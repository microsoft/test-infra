# Deploying Open Enclave Test Infra

This document will walk you through deploying your own Prow instance to a new AKS Kubernetes cluster.

For all other cloud providers please follow the Prow [documentation](https://github.com/kubernetes/test-infra/blob/master/prow/getting_started_deploy.md). The below documentation is copy/pasted for some bits but is also Azure specific as it is currently the only provider of cloud SGX hardware which is required for the openenclave repos and requires additional set up. If you are specifically trying to make an SGX AKS cluster, please read on.

# GitHub bot account

Ensure you have a Github account for Prow to use. Prow will ignore most GitHub events generated by this account, so it is important this account be separate from any users or automation you wish to interact with prow. For example, you still need to do this even if you'd just setting up a prow instance to work against your own personal repos.

1. Ensure the bot user has the following permissions
    - Write access to the repos you plan on handling
    - Owner access (and org membership) for the orgs you plan on handling (note
      it is possible to handle specific repos in an org without this)
1. Create a [personal access token][1] for the GitHub bot account, adding the
   following scopes (more details [here][8])
    - Must have the `public_repo` and `repo:status` scopes
    - Add the `repo` scope if you plan on handing private repos
    - Add the `admin:org_hook` scope if you plan on handling a github org
1. Set this token aside for later (we'll assume you wrote it to a file on your
   workstation at `/path/to/oauth/secret`)

# Install dependencies

## General
```
sudo apt install -y python3-pip curl git gnupg

```
## Install Azure CLI
```
curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash
```

## Install Kubectl
``` 
curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin/kubectl
kubectl version --client
```
## Install Helm
```
curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | sudo bash
```

## Install Bazel
```
curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add -
echo "deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8" | sudo tee /etc/apt/sources.list.d/bazel.list
sudo apt update && sudo apt install bazel-3.0.0 -y
```
## Get Prow tools
```
git clone https://github.com/kubernetes/test-infra.git ~/prow-tools
```

# Deploy Cluster
## Login to Azure
```
az login
```

## List all subscriptions

```
az account list
```

## Set Subscription
```
az account set --subscription "${SUBSCRIPTION_NAME}"
```

## Create AKS RBAC Service Principal
```
az ad sp create-for-rbac --skip-assignment --name "${SERVICE_PRINCIPAL_NAME}"
export SERVICE_PRINCIPAL= # appid output from above
export CLIENT_SECRET= # password output from above
```

## Use some default values to get started

```
export LOCATION="westus2"
export RESOURCE_GROUP="OpenEnclaveProws"
export AKS_CLUSTER_NAME="oe-prow-prod-primary"
export NODE_SIZE="Standard_D2s_v3"
export MIN_NODE_COUNT="15"
export MAX_NODE_COUNT="100"
export PATH_KEY="~/.ssh/id_rsa.pub"
export DNS_LABEL="oe-prow-status"
```
## Create Resource Group
```
az group create --location ${LOCATION} --name ${RESOURCE_GROUP}
```

## Create AKS Cluster with non-ACCNodes

Run the following to create the cluster.
```
az aks create --resource-group ${RESOURCE_GROUP} \
    --name ${AKS_CLUSTER_NAME} \
    --node-vm-size ${NODE_SIZE} \
    --vm-set-type VirtualMachineScaleSets \
    --load-balancer-sku standard \
    --location ${LOCATION} \
    --enable-cluster-autoscaler \
    --service-principal ${SERVICE_PRINCIPAL} \
    --client-secret ${CLIENT_SECRET} \
    --min-count ${MIN_NODE_COUNT} \
    --node-count ${MIN_NODE_COUNT} \
    --max-count ${MAX_NODE_COUNT} \
    --network-plugin azure \
    --ssh-key-value ${PATH_KEY} \
    --zones 1 2 3
```

# Configure Cluster

## Get Credentials

This will also set up kubectl to point to the new cluster.
```
az aks get-credentials --resource-group ${RESOURCE_GROUP} --name ${AKS_CLUSTER_NAME} --overwrite-existing
```

## Create a namespace for your ingress resources
```
kubectl create namespace ingress-basic
```

## Add the official stable repository
```
helm repo add stable https://charts.helm.sh/stable
```

## Assign Public IP
```
# Get AKS Resource Group Name
AKS_RESOURCE_GROUP=$(az aks show --resource-group ${RESOURCE_GROUP} --name ${AKS_CLUSTER_NAME} --query nodeResourceGroup -o tsv)
echo ${AKS_RESOURCE_GROUP}

# Assign Static IP
STATIC_IP=$(az network public-ip create --resource-group ${AKS_RESOURCE_GROUP} --name myAKSPublicIP --sku Standard --allocation-method static --query publicIp.ipAddress -o tsv --dns-name ${DNS_LABEL})
echo ${STATIC_IP}

#Get FQDN
az network public-ip list --resource-group ${AKS_RESOURCE_GROUP} --query "[?name=='myAKSPublicIP'].[dnsSettings.fqdn]" -o tsv
```

## Use Helm to deploy an NGINX ingress controller
```
helm install nginx stable/nginx-ingress \
    --namespace ingress-basic \
    --set controller.replicaCount=5 \
    --set controller.nodeSelector."beta\.kubernetes\.io/os"=linux \
    --set controller.service.loadBalancerIP="${STATIC_IP}" \
    --set defaultBackend.nodeSelector."beta\.kubernetes\.io/os"=linux \
    --set rbac.create=true
```

## Create cluster role bindings

As of 1.8 Kubernetes uses Role-Based Access Control (“RBAC”) to drive authorization decisions, allowing cluster-admin to dynamically configure policies. To create cluster resources you need to grant a user cluster-admin role in all namespaces for the cluster.

```
kubectl create clusterrolebinding cluster-admin-binding-"${USER}" \
  --clusterrole=cluster-admin --user="${USER}"
```

## Set namespaces for prowjobs and test pods
```
kubectl create namespace test-pods
```

## Create GH secrets

You will need two secrets to talk to GitHub. The hmac-token is the token that you give to GitHub for validating webhooks. Generate it using any reasonable randomness-generator, eg openssl rand -hex 20

```
openssl rand -hex 20 > $PWD/hmac
kubectl create secret generic hmac-token --from-file=$PWD/hmac
```

The oauth-token is the OAuth2 token you created above for the [GitHub bot account]. If you need to create one, go to https://github.com/settings/tokens.

```
kubectl create secret generic oauth-token --from-file=$PWD/oauth
```
## Configure Cloud storage (optional)

This is done on GCS as currently prow only integrates with GCS. Follow this [guide](https://github.com/kubernetes/test-infra/blob/master/prow/getting_started_deploy.md#configure-cloud-storage).

Set up can be read about [here](https://github.com/kubernetes/test-infra/blob/master/prow/getting_started_deploy.md#configure-cloud-storage).
```
kubectl -n test-pods create secret generic gcs-credentials --from-file=service-account.json
```

## Add dummy credentials
We use Jenkins, slack etc in the backend unless you are using them just set as dummy creds if they have values run the following:
```
# Generate a dummy credential even if you are not using Jenkins or else some prow jobs will fail
kubectl -n test-pods create secret generic jenkins-token --from-file=jenkins-token=$PWD/jenkins-secret

# Generate a dummy credential even if you are not using Docker or else some prow jobs will fail
kubectl -n test-pods create secret generic docker-password --from-file=docker-password=$PWD/docker-creds

# Generate and configure PR Status
kubectl create secret generic github-oauth-config --from-file=secret=$PWD/pr-status
kubectl create secret generic cookie --from-file=secret=$PWD/cookie.txt

# Generate Slack integration
kubectl create secret generic slack-token --from-literal=token=$PWD/slack-reporter
```

## Install cert-manager (optional)
This is needed if TLS needs to be enabled
```
# Label the ingress-basic namespace to disable resource validation
kubectl label namespace ingress-basic cert-manager.io/disable-validation=true

# Add the Jetstack Helm repository
helm repo add jetstack https://charts.jetstack.io

# Update your local Helm chart repository cache
helm repo update

# Install the cert-manager Helm chart
helm install \
  cert-manager \
  --namespace ingress-basic \
  --version v0.16.1 \
  --set installCRDs=true \
  --set nodeSelector."beta\.kubernetes\.io/os"=linux \
  jetstack/cert-manager
```

## Add the prow components to the cluster

Run the following command to deploy a basic set of prow components. Chang the dns at ing_ingress first to your dns label if you need TLS
```
# Deploy TLS cluster issuer and wait for 2 minutes, patience will save you hours of debugging..
kubectl apply -f config/prow/cluster/cluster-issuer.yaml

kubectl apply -f config/prow/cluster/configs.yaml
kubectl apply -f config/prow/cluster/hook_deployment.yaml
kubectl apply -f config/prow/cluster/hook_service.yaml
kubectl apply -f config/prow/cluster/plank_deployment.yaml
kubectl apply -f config/prow/cluster/sinker_deployment.yaml
kubectl apply -f config/prow/cluster/deck_deployment.yaml
kubectl apply -f config/prow/cluster/deck_service.yaml
kubectl apply -f config/prow/cluster/horolgium_deployment.yaml
kubectl apply -f config/prow/cluster/tide_deployment.yaml
kubectl apply -f config/prow/cluster/tide_service.yaml
kubectl apply -f config/prow/cluster/ing_ingress.yaml
kubectl apply -f config/prow/cluster/statusreconciler_deployment.yaml
kubectl apply -f config/prow/cluster/test_pods.yaml
kubectl apply -f config/prow/cluster/deck_rbac.yaml
kubectl apply -f config/prow/cluster/horolgium_rbac.yaml
kubectl apply -f config/prow/cluster/plank_rbac.yaml
kubectl apply -f config/prow/cluster/sinker_rbac.yaml
kubectl apply -f config/prow/cluster/hook_rbac.yaml
kubectl apply -f config/prow/cluster/tide_rbac.yaml
kubectl apply -f config/prow/cluster/statusreconciler_rbac.yaml
kubectl apply -f config/prow/cluster/crier_rbac.yaml
kubectl apply -f config/prow/cluster/crier_deployment.yaml
```

## Check deployment status
```
kubectl get deployments -w
```

You should see the below after ~10 minutes. The initialization takes some time to pull all the containers.
```
crier              1/1     1            1           127m
deck               2/2     2            2           128m
hook               2/2     2            2           128m
horologium         1/1     1            1           128m
plank              1/1     1            1           128m
sinker             1/1     1            1           128m
statusreconciler   1/1     1            1           128m
tide               1/1     1            1           128m
```

If this is not the case, and you are stuck on "container creating", I suggest

```
kubectl describe pods
```

which will give you the error usually

```
Events:
  Type     Reason       Age                   From               Message
  ----     ------       ----                  ----               -------
  Normal   Scheduled    9m47s                 default-scheduler  Successfully assigned default/tide-6dc6784468-w9h5c to aks-nodepool1-42367106-vmss000000
  Warning  FailedMount  3m11s                 kubelet            Unable to attach or mount volumes: unmounted volumes=[oauth], unattached volumes=[tide-token-hbj9z oauth config job-config]: timed out waiting for the condition
  Warning  FailedMount  92s (x12 over 9m46s)  kubelet            MountVolume.SetUp failed for volume "oauth" : secret "oauth-token" not found
  Warning  FailedMount  56s (x3 over 7m43s)   kubelet            Unable to attach or mount volumes: unmounted volumes=[oauth], unattached volumes=[oauth config job-config tide-token-hbj9z]: timed out waiting for the condition
```

## Configure plugins and.. well, config!
```
kubectl create configmap config --from-file=config.yaml=$PWD/config/prow/primary/config.yaml  --dry-run=client -o yaml | kubectl replace configmap config -f -
kubectl create configmap plugins --from-file=$PWD/config/prow/primary/plugins.yaml --dry-run=client -o yaml   | kubectl replace configmap plugins -f -
```

## Get some jobs going to sanity check
```
# Create job config map
kubectl create configmap job-config \
--from-file=test-infra-periodics.yaml=$PWD/config/jobs/test-infra/test-infra-periodics.yaml \
--from-file=test-infra-postsubmits.yaml=$PWD/config/jobs/test-infra/test-infra-postsubmits.yaml \
--from-file=test-infra-pre-submits.yaml=$PWD/config/jobs/test-infra/test-infra-pre-submits.yaml \
--from-file=oeedger8r-cpp-pre-submits.yaml=$PWD/config/jobs/oeedger8r-cpp/oeedger8r-cpp-pre-submits.yaml \
--from-file=oeedger8r-cpp-periodics.yaml=$PWD/config/jobs/oeedger8r-cpp/oeedger8r-cpp-periodics.yaml \
--from-file=oeedger8r-cpp-postsubmits.yaml=$PWD/config/jobs/oeedger8r-cpp/oeedger8r-cpp-postsubmits.yaml \
--from-file=openenclave-periodics.yaml=$PWD/config/jobs/openenclave/openenclave-periodics.yaml \
--from-file=openenclave-pre-submits.yaml=$PWD/config/jobs/openenclave/openenclave-pre-submits.yaml \
--from-file=openenclave-postsubmits.yaml=$PWD/config/jobs/openenclave/openenclave-postsubmits.yaml \
--from-file=openenclave-mbedtls-periodics.yaml=$PWD/config/jobs/openenclave-mbedtls/openenclave-mbedtls-periodics.yaml \
--from-file=openenclave-mbedtls-pre-submits.yaml=$PWD/config/jobs/openenclave-mbedtls/openenclave-mbedtls-pre-submits.yaml \
--from-file=openenclave-mbedtls-postsubmits.yaml=$PWD/config/jobs/openenclave-mbedtls/openenclave-mbedtls-postsubmits.yaml \
--from-file=openenclave-curl-periodics.yaml=$PWD/config/jobs/openenclave-curl/openenclave-curl-periodics.yaml \
--from-file=openenclave-curl-pre-submits.yaml=$PWD/config/jobs/openenclave-curl/openenclave-curl-pre-submits.yaml \
--from-file=openenclave-curl-postsubmits.yaml=$PWD/config/jobs/openenclave-curl/openenclave-curl-postsubmits.yaml \
--from-file=openenclave-ci-pre-submits.yaml=$PWD/config/jobs/openenclave-ci/openenclave-ci-pre-submits.yaml \
--dry-run=client -o yaml | kubectl replace configmap job-config -f -
```

## Take a coffee break..

Prow will fallover instantly as you just gave it about 200 jobs to run at the same second. That is not an issue, just wait for them to fail the first time and subsequent requests will succeed. You generally need the cluster to be up for 30 minutes before considering it stable due to how GitHubs clone process works wrt api key caching. Watch the progress at your GQDN and you are nearly there, wait until you are not constantly see the below failure before expecting it to work.

```
# FAILED!
# Cloning openenclave/test-infra at master
$ mkdir -p /home/prow/go/src/github.com/openenclave/test-infra
$ git init
Initialized empty Git repository in /home/prow/go/src/github.com/openenclave/test-infra/.git/
$ git config user.name ci-robot
$ git config user.email ci-robot@k8s.io
$ git fetch https://github.com/openenclave/test-infra.git --tags --prune
fatal: unable to access 'https://github.com/openenclave/test-infra.git/': Could not resolve host: github.com
# Error: exit status 128
```

## Configure

Take a look over at config options [options](row-config-documented.yaml) for the spread of all available options.

## Clean up
```
az group delete --name ${RESOURCE_GROUP} --yes
az group delete --name MC_${RESOURCE_GROUP}_oe-${AKS_CLUSTER}_${LOCATION} --yes
```